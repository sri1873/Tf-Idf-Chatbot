{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "dataset={\n",
        "   \"intents\":[\n",
        " {\n",
        "   \"tag\": \"greeting\",\n",
        "   \"patterns\": [\n",
        "      \"Hi\",\n",
        "      \"How are you?\",\n",
        "      \"Is anyone there?\",\n",
        "      \"Hello\",\n",
        "      \"Good day\",\n",
        "      \"What's up\",\n",
        "      \"how are ya\",\n",
        "      \"heyy\",\n",
        "      \"whatsup\",\n",
        "      \"??? ??? ??\"\n",
        "   ],\n",
        "   \"responses\": [\n",
        "      \"Hello!\",\n",
        "      \"Good to see you again!\",\n",
        "      \"Hi there, how can I help?\"\n",
        "   ],\n",
        "   \"context_set\": \"\"\n",
        "},\n",
        " {\n",
        "   \"tag\": \"goodbye\",\n",
        "   \"patterns\": [\n",
        "      \"cya\",\n",
        "      \"see you\",\n",
        "      \"bye bye\",\n",
        "      \"See you later\",\n",
        "      \"Goodbye\",\n",
        "      \"I am Leaving\",\n",
        "      \"Bye\",\n",
        "      \"Have a Good day\",\n",
        "      \"talk to you later\",\n",
        "      \"ttyl\",\n",
        "      \"i got to go\",\n",
        "      \"gtg\"\n",
        "   ],\n",
        "   \"responses\": [\n",
        "      \"Sad to see you go :(\",\n",
        "      \"Talk to you later\",\n",
        "      \"Goodbye!\",\n",
        "      \"Come back soon\"\n",
        "   ],\n",
        "   \"context_set\": \"\"\n",
        "},\n",
        " {\n",
        "   \"tag\": \"creator\",\n",
        "   \"patterns\": [\n",
        "      \"what is the name of your developers\",\n",
        "      \"what is the name of your creators\",\n",
        "      \"what is the name of the developers\",\n",
        "      \"what is the name of the creators\",\n",
        "      \"who created you\",\n",
        "      \"your developers\",\n",
        "      \"your creators\",\n",
        "      \"who are your developers\",\n",
        "      \"developers\",\n",
        "      \"you are made by\",\n",
        "      \"you are made by whom\",\n",
        "      \"who created you\",\n",
        "      \"who create you\",\n",
        "      \"creators\",\n",
        "      \"who made you\",\n",
        "      \"who designed you\"\n",
        "   ],\n",
        "   \"responses\": [\n",
        "      \"College students\"\n",
        "   ],\n",
        "   \"context_set\": \"\"\n",
        "},\n",
        " {\n",
        "   \"tag\": \"name\",\n",
        "   \"patterns\": [\n",
        "      \"name\",\n",
        "      \"your name\",\n",
        "      \"do you have a name\",\n",
        "      \"what are you called\",\n",
        "      \"what is your name\",\n",
        "      \"what should I call you\",\n",
        "      \"whats your name?\",\n",
        "      \"what are you\",\n",
        "      \"who are you\",\n",
        "      \"who is this\",\n",
        "      \"what am i chatting to\",\n",
        "      \"who am i taking to\",\n",
        "      \"what are you\"\n",
        "   ],\n",
        "   \"responses\": [\n",
        "      \"You can call me Mind Reader.\",\n",
        "      \"I'm Mind Reader\",\n",
        "      \"I am a Chatbot.\",\n",
        "      \"I am your helper\"\n",
        "   ],\n",
        "   \"context_set\": \"\"\n",
        "}\n",
        "]\n",
        "}"
      ],
      "metadata": {
        "id": "0lxYor4l2GNf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_query=input().lower().split()\n",
        "tags=[]\n",
        "doc_count=len(dataset[\"intents\"])\n",
        "for i in dataset[\"intents\"]:\n",
        "    if i['tag'].lower() in input_query:\n",
        "        tags.append(i)\n",
        "print(doc_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NZkwOsq1zDM",
        "outputId": "7fb809ec-6d56-4f44-c866-dc604adb429c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do i call you\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "def extract_ngrams(sentence, n):\n",
        "    ngrams = [[] for _ in range(n)]\n",
        "    for i in range(len(sentence)):\n",
        "        for j in range(1, n + 1):\n",
        "            if i + j <= len(sentence):\n",
        "                ngram = \" \".join(sentence[i : i + j])\n",
        "                ngrams[j - 1].append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "inp_grams=extract_ngrams(input_query,3 if len(input_query)>3 else len(input_query))\n",
        "inp_df=Counter()\n",
        "for gram in inp_grams:\n",
        "    inp_df.update(gram)\n",
        "inp_tfidf=[]\n",
        "for gram in inp_grams:\n",
        "    _tfidf=[]\n",
        "    for word in gram:\n",
        "        tf=gram.count(word)/len(gram)\n",
        "        idf=(math.log(doc_count/inp_df[word]))\n",
        "        tf_idf=tf*idf\n",
        "        _tfidf.append(tf_idf)\n",
        "    inp_tfidf.append(_tfidf)\n",
        "print(inp_tfidf)\n",
        "print(inp_grams)\n",
        "\n",
        "\n",
        "df=Counter()\n",
        "for i in dataset['intents']:\n",
        "    for sent in i['patterns']:\n",
        "        sent_gram=(extract_ngrams(sent.lower().split(),3 if len(sent.split())>3 else len(sent.split())))\n",
        "        for gram in sent_gram:\n",
        "            df.update(gram)\n",
        "\n",
        "unigram_tfidf=[]\n",
        "bigram_tfidf=[]\n",
        "trigram_tfidf=[]\n",
        "\n",
        "for i in dataset['intents']:\n",
        "    for sent in i['patterns']:\n",
        "        sent_gram=(extract_ngrams(sent.lower().split(),3 if len(sent.split())>3 else len(sent.split())))\n",
        "        print(sent_gram)\n",
        "        gram_tfidf=[]\n",
        "        for gram in sent_gram:\n",
        "            word_tfidf=[]\n",
        "            for word in gram:\n",
        "                tf=gram.count(word)/len(gram)\n",
        "                idf=(math.log(doc_count/df[word]))\n",
        "                tf_idf=tf*idf\n",
        "                word_tfidf.append(tf_idf)\n",
        "            gram_tfidf.append(word_tfidf)\n",
        "        unigram_tfidf.append(gram_tfidf[0])\n",
        "        if (len(gram_tfidf)==2):\n",
        "            bigram_tfidf.append(gram_tfidf[1])\n",
        "        if (len(gram_tfidf)==3):\n",
        "            trigram_tfidf.append(gram_tfidf[2])\n",
        "\n",
        "print(unigram_tfidf)\n",
        "print(bigram_tfidf)\n",
        "print(trigram_tfidf)\n",
        "print(inp_tfidf)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "483lNfMe1mqB",
        "outputId": "c59d09d6-e6c2-4721-da8e-85397487361e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.2772588722239781, 0.2772588722239781, 0.2772588722239781, 0.2772588722239781, 0.2772588722239781], [0.34657359027997264, 0.34657359027997264, 0.34657359027997264, 0.34657359027997264], [0.46209812037329684, 0.46209812037329684, 0.46209812037329684]]\n",
            "[['what', 'do', 'i', 'call', 'you'], ['what do', 'do i', 'i call', 'call you'], ['what do i', 'do i call', 'i call you']]\n",
            "[['hi']]\n",
            "[['how', 'are', 'you?'], ['how are', 'are you?'], ['how are you?']]\n",
            "[['is', 'anyone', 'there?'], ['is anyone', 'anyone there?'], ['is anyone there?']]\n",
            "[['hello']]\n",
            "[['good', 'day'], ['good day']]\n",
            "[[\"what's\", 'up'], [\"what's up\"]]\n",
            "[['how', 'are', 'ya'], ['how are', 'are ya'], ['how are ya']]\n",
            "[['heyy']]\n",
            "[['whatsup']]\n",
            "[['???', '???', '??'], ['??? ???', '??? ??'], ['??? ??? ??']]\n",
            "[['cya']]\n",
            "[['see', 'you'], ['see you']]\n",
            "[['bye', 'bye'], ['bye bye']]\n",
            "[['see', 'you', 'later'], ['see you', 'you later'], ['see you later']]\n",
            "[['goodbye']]\n",
            "[['i', 'am', 'leaving'], ['i am', 'am leaving'], ['i am leaving']]\n",
            "[['bye']]\n",
            "[['have', 'a', 'good', 'day'], ['have a', 'a good', 'good day'], ['have a good', 'a good day']]\n",
            "[['talk', 'to', 'you', 'later'], ['talk to', 'to you', 'you later'], ['talk to you', 'to you later']]\n",
            "[['ttyl']]\n",
            "[['i', 'got', 'to', 'go'], ['i got', 'got to', 'to go'], ['i got to', 'got to go']]\n",
            "[['gtg']]\n",
            "[['what', 'is', 'the', 'name', 'of', 'your', 'developers'], ['what is', 'is the', 'the name', 'name of', 'of your', 'your developers'], ['what is the', 'is the name', 'the name of', 'name of your', 'of your developers']]\n",
            "[['what', 'is', 'the', 'name', 'of', 'your', 'creators'], ['what is', 'is the', 'the name', 'name of', 'of your', 'your creators'], ['what is the', 'is the name', 'the name of', 'name of your', 'of your creators']]\n",
            "[['what', 'is', 'the', 'name', 'of', 'the', 'developers'], ['what is', 'is the', 'the name', 'name of', 'of the', 'the developers'], ['what is the', 'is the name', 'the name of', 'name of the', 'of the developers']]\n",
            "[['what', 'is', 'the', 'name', 'of', 'the', 'creators'], ['what is', 'is the', 'the name', 'name of', 'of the', 'the creators'], ['what is the', 'is the name', 'the name of', 'name of the', 'of the creators']]\n",
            "[['who', 'created', 'you'], ['who created', 'created you'], ['who created you']]\n",
            "[['your', 'developers'], ['your developers']]\n",
            "[['your', 'creators'], ['your creators']]\n",
            "[['who', 'are', 'your', 'developers'], ['who are', 'are your', 'your developers'], ['who are your', 'are your developers']]\n",
            "[['developers']]\n",
            "[['you', 'are', 'made', 'by'], ['you are', 'are made', 'made by'], ['you are made', 'are made by']]\n",
            "[['you', 'are', 'made', 'by', 'whom'], ['you are', 'are made', 'made by', 'by whom'], ['you are made', 'are made by', 'made by whom']]\n",
            "[['who', 'created', 'you'], ['who created', 'created you'], ['who created you']]\n",
            "[['who', 'create', 'you'], ['who create', 'create you'], ['who create you']]\n",
            "[['creators']]\n",
            "[['who', 'made', 'you'], ['who made', 'made you'], ['who made you']]\n",
            "[['who', 'designed', 'you'], ['who designed', 'designed you'], ['who designed you']]\n",
            "[['name']]\n",
            "[['your', 'name'], ['your name']]\n",
            "[['do', 'you', 'have', 'a', 'name'], ['do you', 'you have', 'have a', 'a name'], ['do you have', 'you have a', 'have a name']]\n",
            "[['what', 'are', 'you', 'called'], ['what are', 'are you', 'you called'], ['what are you', 'are you called']]\n",
            "[['what', 'is', 'your', 'name'], ['what is', 'is your', 'your name'], ['what is your', 'is your name']]\n",
            "[['what', 'should', 'i', 'call', 'you'], ['what should', 'should i', 'i call', 'call you'], ['what should i', 'should i call', 'i call you']]\n",
            "[['whats', 'your', 'name?'], ['whats your', 'your name?'], ['whats your name?']]\n",
            "[['what', 'are', 'you'], ['what are', 'are you'], ['what are you']]\n",
            "[['who', 'are', 'you'], ['who are', 'are you'], ['who are you']]\n",
            "[['who', 'is', 'this'], ['who is', 'is this'], ['who is this']]\n",
            "[['what', 'am', 'i', 'chatting', 'to'], ['what am', 'am i', 'i chatting', 'chatting to'], ['what am i', 'am i chatting', 'i chatting to']]\n",
            "[['who', 'am', 'i', 'taking', 'to'], ['who am', 'am i', 'i taking', 'taking to'], ['who am i', 'am i taking', 'i taking to']]\n",
            "[['what', 'are', 'you'], ['what are', 'are you'], ['what are you']]\n",
            "[[1.3862943611198906], [0.23104906018664842, -0.27031007207210955, 0.46209812037329684], [-0.18653859597847425, 0.46209812037329684, 0.46209812037329684], [1.3862943611198906], [0.34657359027997264, 0.34657359027997264], [0.6931471805599453, 0.6931471805599453], [0.23104906018664842, -0.27031007207210955, 0.46209812037329684], [1.3862943611198906], [1.3862943611198906], [0.46209812037329684, 0.46209812037329684, 0.46209812037329684], [1.3862943611198906], [0.34657359027997264, -0.6931471805599453], [0.28768207245178085, 0.28768207245178085], [0.23104906018664842, -0.46209812037329684, 0.23104906018664842], [1.3862943611198906], [-0.07438118377140324, 0.09589402415059362, 0.46209812037329684], [0.28768207245178085], [0.17328679513998632, 0.17328679513998632, 0.17328679513998632, 0.17328679513998632], [0.34657359027997264, 0.0, -0.34657359027997264, 0.17328679513998632], [1.3862943611198906], [-0.05578588782855243, 0.34657359027997264, 0.0, 0.34657359027997264], [1.3862943611198906], [-0.13089867598202212, -0.07994511256220325, -0.05792358687259492, -0.09902102579427789, 0.0, -0.09902102579427789, -0.031877650187744244], [-0.13089867598202212, -0.07994511256220325, -0.05792358687259492, -0.09902102579427789, 0.0, -0.09902102579427789, 0.0], [-0.13089867598202212, -0.07994511256220325, -0.11584717374518984, -0.09902102579427789, 0.0, -0.11584717374518984, -0.031877650187744244], [-0.13089867598202212, -0.07994511256220325, -0.11584717374518984, -0.09902102579427789, 0.0, -0.11584717374518984, 0.0], [-0.27031007207210955, 0.23104906018664842, -0.46209812037329684], [-0.34657359027997264, -0.11157177565710485], [-0.34657359027997264, 0.0], [-0.2027325540540822, -0.2027325540540822, -0.17328679513998632, -0.05578588782855243], [-0.2231435513142097], [-0.34657359027997264, -0.2027325540540822, 0.07192051811294521, 0.17328679513998632], [-0.2772588722239781, -0.16218604324326577, 0.05753641449035617, 0.13862943611198905, 0.2772588722239781], [-0.27031007207210955, 0.23104906018664842, -0.46209812037329684], [-0.27031007207210955, 0.46209812037329684, -0.46209812037329684], [0.0], [-0.27031007207210955, 0.09589402415059362, -0.46209812037329684], [-0.27031007207210955, 0.46209812037329684, -0.46209812037329684], [-0.6931471805599453], [-0.34657359027997264, -0.34657359027997264], [0.2772588722239781, -0.2772588722239781, 0.13862943611198905, 0.13862943611198905, -0.13862943611198905], [-0.22907268296853875, -0.2027325540540822, -0.34657359027997264, 0.34657359027997264], [-0.22907268296853875, -0.1399039469838557, -0.17328679513998632, -0.17328679513998632], [-0.183258146374831, 0.2772588722239781, -0.044628710262841945, 0.2772588722239781, -0.2772588722239781], [0.46209812037329684, -0.23104906018664842, 0.46209812037329684], [-0.30543024395805163, -0.27031007207210955, -0.46209812037329684], [-0.27031007207210955, -0.27031007207210955, -0.46209812037329684], [-0.27031007207210955, -0.18653859597847425, 0.46209812037329684], [-0.183258146374831, 0.05753641449035617, -0.044628710262841945, 0.2772588722239781, 0.0], [-0.16218604324326577, 0.05753641449035617, -0.044628710262841945, 0.2772588722239781, 0.0], [-0.30543024395805163, -0.27031007207210955, -0.46209812037329684]]\n",
            "[[0.6931471805599453], [1.3862943611198906], [0.6931471805599453], [1.3862943611198906], [0.28768207245178085], [0.6931471805599453], [0.6931471805599453]]\n",
            "[[1.3862943611198906], [1.3862943611198906], [1.3862943611198906], [1.3862943611198906], [1.3862943611198906], [1.3862943611198906], [0.6931471805599453, 0.6931471805599453], [0.6931471805599453, 0.6931471805599453], [0.6931471805599453, 0.6931471805599453], [0.0, 0.0, 0.0, 0.13862943611198905, 0.2772588722239781], [0.0, 0.0, 0.0, 0.13862943611198905, 0.2772588722239781], [0.0, 0.0, 0.0, 0.13862943611198905, 0.2772588722239781], [0.0, 0.0, 0.0, 0.13862943611198905, 0.2772588722239781], [0.6931471805599453], [0.6931471805599453, 0.6931471805599453], [0.34657359027997264, 0.34657359027997264], [0.23104906018664842, 0.23104906018664842, 0.46209812037329684], [0.6931471805599453], [1.3862943611198906], [1.3862943611198906], [1.3862943611198906], [0.46209812037329684, 0.46209812037329684, 0.46209812037329684], [0.14384103622589042, 0.6931471805599453], [0.6931471805599453, 0.6931471805599453], [0.46209812037329684, 0.46209812037329684, 0.46209812037329684], [1.3862943611198906], [0.28768207245178085], [1.3862943611198906], [1.3862943611198906], [0.46209812037329684, 0.46209812037329684, 0.46209812037329684], [0.46209812037329684, 0.46209812037329684, 0.46209812037329684], [0.28768207245178085]]\n",
            "[[0.2772588722239781, 0.2772588722239781, 0.2772588722239781, 0.2772588722239781, 0.2772588722239781], [0.34657359027997264, 0.34657359027997264, 0.34657359027997264, 0.34657359027997264], [0.46209812037329684, 0.46209812037329684, 0.46209812037329684]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1, vec2))\n",
        "    magnitude1 = math.sqrt(sum(v ** 2 for v in vec1))\n",
        "    magnitude2 = math.sqrt(sum(v ** 2 for v in vec2))\n",
        "    if magnitude1 == 0 or magnitude2 == 0:\n",
        "        return 0\n",
        "    return dot_product / (magnitude1 * magnitude2)\n",
        "\n",
        "unigram_similarities = []\n",
        "for tfidf in unigram_tfidf:\n",
        "    similarity = cosine_similarity(tfidf, inp_tfidf[0])\n",
        "    unigram_similarities.append(similarity)\n",
        "\n",
        "bigram_similarities = []\n",
        "for tfidf in bigram_tfidf:\n",
        "    similarity = cosine_similarity(tfidf, inp_tfidf[1])\n",
        "    bigram_similarities.append(similarity)\n",
        "\n",
        "trigram_similarities = []\n",
        "for tfidf in trigram_tfidf:\n",
        "    similarity = cosine_similarity(tfidf, inp_tfidf[2])\n",
        "    trigram_similarities.append(similarity)\n",
        "\n",
        "print(\"Unigram Similarities:\", unigram_similarities)\n",
        "print(\"Bigram Similarities:\", bigram_similarities)\n",
        "print(\"Trigram Similarities:\", trigram_similarities)\n"
      ],
      "metadata": {
        "id": "Db_eiC_mqoWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad5c009-733b-4d5e-f58a-fc03badc54d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Similarities: [0.4472135954999579, 0.32430799427172935, 0.4854135016629071, 0.4472135954999579, 0.6324555320336759, 0.6324555320336759, 0.32430799427172935, 0.4472135954999579, 0.4472135954999579, 0.7745966692414832, 0.4472135954999579, -0.19999999999999998, 0.6324555320336759, 0.0, 0.4472135954999579, 0.4526822549120389, 0.44721359549995787, 0.8944271909999159, 0.14907119849998599, 0.4472135954999579, 0.5778235060023763, 0.4472135954999579, -0.7546275888656482, -0.7628301792506159, -0.7696568752774089, -0.7761286493143504, -0.38453288826548215, -0.562742307652138, -0.4472135954999579, -0.8355644700328977, -0.4472135954999579, -0.306861813420777, 0.03376291580776861, -0.38453288826548215, -0.1709357042369696, 0, -0.5233901564490056, -0.1709357042369696, -0.4472135954999579, -0.6324555320336759, 0.1348399724926484, -0.3342403658751178, -0.8804345478695227, 0.04279546710560777, 0.44721359549995787, -0.7530362937789835, -0.7477251202069216, 0.00414101525510829, 0.14052418781502932, 0.17377454713371018, -0.7530362937789835]\n",
            "Bigram Similarities: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
            "Trigram Similarities: [0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 0.816496580927726, 0.816496580927726, 0.816496580927726, 0.0, 0.0, 0.0, 0.0, 0.5773502691896257, 0.816496580927726, 0.816496580927726, 0.9428090415820634, 0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 1.0, 0.6826180545840519, 0.816496580927726, 1.0, 0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 0.5773502691896257, 1.0, 1.0, 0.5773502691896257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_similarities=[]\n",
        "max_len =(max(len(unigram_similarities),len(bigram_similarities),len(trigram_similarities)))\n",
        "\n",
        "unigram_similarities += [0] * (max_len - len(unigram_similarities))\n",
        "bigram_similarities += [0] * (max_len - len(bigram_similarities))\n",
        "trigram_similarities += [0] * (max_len - len(trigram_similarities))\n",
        "\n",
        "for i,j,k in zip(unigram_similarities,bigram_similarities,trigram_similarities):\n",
        "    avg_similarities.append((i+j+k)/3)\n",
        "print(avg_similarities)\n",
        "\n",
        "max(avg_similarities)\n",
        "\n",
        "avg_similarities.index(max(avg_similarities))"
      ],
      "metadata": {
        "id": "9dUgg84ekfeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98179f7a-f5bc-4e7e-cb2a-ab530cc80ce5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5081879548965279, 0.467219421153785, 0.5209212569508442, 0.5081879548965279, 0.5699352670744339, 0.5699352670744339, 0.5469348583998185, 0.42123672547589464, 0.42123672547589464, 0.25819888974716104, 0.14907119849998599, -0.06666666666666667, 0.21081851067789195, 0.19245008972987523, 0.42123672547589464, 0.4230596119465883, 0.4633408790273404, 0.4905924867298472, 0.24214048922987055, 0.3415212882298612, 0.3850579250640007, 0.48240453183331927, -0.02400317809386543, 0.017888800559036706, 0.07678104157419703, -0.0662594600415749, 0.06427246030804785, 0.004869320512495927, 0.04337889122988927, 0.05481184332236744, 0.18426213483334739, 0.09016281858961624, 0.011254305269256205, -0.1281776294218274, -0.05697856807898987, 0.0, -0.17446338548300186, -0.05697856807898987, -0.14907119849998599, -0.21081851067789195, 0.04494665749754947, -0.11141345529170593, -0.2934781826231742, 0.014265155701869257, 0.14907119849998596, -0.25101209792632784, -0.24924170673564053, 0.00138033841836943, 0.046841395938343106, 0.05792484904457006, -0.25101209792632784]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_dataset(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def compute_tf_idf(docs):\n",
        "\n",
        "    tf = {}\n",
        "    df = {}\n",
        "    for doc in docs:\n",
        "        tf[doc] = {}\n",
        "        for word in set(docs[doc]):\n",
        "            tf[doc][word] = docs[doc].count(word)\n",
        "\n",
        "            df[word] = df.get(word, 0) + 1\n",
        "\n",
        "\n",
        "    tf_idf = {}\n",
        "    N = len(docs)\n",
        "    for doc in docs:\n",
        "        tf_idf[doc] = {}\n",
        "        for word in tf[doc]:\n",
        "            tf_idf[doc][word] = tf[doc][word] * np.log(N / df[word])\n",
        "    return tf_idf\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def chatbot_response(query, intents_data, tf_idf):\n",
        "    processed_query = preprocess(query)\n",
        "\n",
        "    query_tf_idf = {}\n",
        "    for word in set(processed_query):\n",
        "        query_tf_idf[word] = processed_query.count(word)\n",
        "\n",
        "\n",
        "    max_similarity = 0\n",
        "    best_intent = None\n",
        "    for intent in intents_data:\n",
        "        intent_tag = intent['tag']\n",
        "        if intent_tag in tf_idf:\n",
        "            similarity = cosine_similarity(tf_idf[intent_tag], query_tf_idf)\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                best_intent = intent_tag\n",
        "\n",
        "    if best_intent:\n",
        "        for intent in intents_data:\n",
        "            if intent['tag'] == best_intent:\n",
        "                return np.random.choice(intent['responses'])\n",
        "    else:\n",
        "        return \"I'm not sure how to respond to that.\"\n",
        "\n",
        "\n",
        "filename = '/content/intents.json'\n",
        "data = load_dataset(filename)\n",
        "\n",
        "\n",
        "docs = {}\n",
        "for intent in data['intents']:\n",
        "    combined_patterns = ' '.join(intent['patterns'])\n",
        "    docs[intent['tag']] = preprocess(combined_patterns)\n",
        "\n",
        "\n",
        "tf_idf = compute_tf_idf(docs)\n",
        "\n",
        "\n",
        "query = input(\"Enter your query: \")\n",
        "response = chatbot_response(query, data['intents'], tf_idf)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFqfDZi2nAD1",
        "outputId": "bf0df71f-d5f4-4cbc-d193-716858df5991"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: you nam\n",
            "I'm Mind Reader\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXRH16KUM_mx",
        "outputId": "5462eeca-8794-4e2f-90be-e9489e28510a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.10.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai) (1.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (4.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.62.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyAJBkjAGESGhwiwaHwET9NTb5rfu6gYnsM\")\n",
        "\n",
        "# Set up the model\n",
        "generation_config = {\n",
        "  \"temperature\": 0.9,\n",
        "  \"top_p\": 1,\n",
        "  \"top_k\": 1,\n",
        "  \"max_output_tokens\": 1000,\n",
        "}\n",
        "\n",
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "]\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings=safety_settings)\n",
        "\n",
        "convo = model.start_chat(history=[\n",
        "])\n",
        "\n",
        "convo.send_message(\"My semester marks in these subjects are following, if i want to become a full stack dev where do i need to improve in every subject, Java 83,Python-80,Html-45\")\n",
        "print(convo.last.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "IF9OYr1jNIXB",
        "outputId": "4307ebae-b10f-4acc-9f8e-f05b2728e9ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Java (83)**\n",
            "\n",
            "* **Improvement areas:**\n",
            "    * Strengthen understanding of core Java concepts (e.g., OOP, data structures, algorithms)\n",
            "    * Gain proficiency in advanced Java features (e.g., generics, lambdas, streams)\n",
            "    * Enhance problem-solving skills through practice with coding challenges and projects\n",
            "\n",
            "**Python (80)**\n",
            "\n",
            "* **Improvement areas:**\n",
            "    * Master data structures, algorithms, and their implementation in Python\n",
            "    * Develop proficiency in object-oriented programming principles\n",
            "    * Strengthen understanding of Python libraries for web development (e.g., Flask, Django)\n",
            "\n",
            "**HTML (45)**\n",
            "\n",
            "* **Significant improvement required:**\n",
            "    * Acquire a comprehensive understanding of HTML elements, attributes, and syntax\n",
            "    * Gain proficiency in creating and structuring web pages with semantic markup\n",
            "    * Practice implementing responsive design for different screen sizes\n",
            "    * Explore CSS and JavaScript to enhance the presentation and functionality of web pages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3TgDXvPPDP",
        "outputId": "9df2c00a-5310-4a64-d7ff-ea86b500824c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n"
          ]
        }
      ]
    }
  ]
}